{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# tatForge Quick Start Guide with CocoIndex\n\nThis notebook demonstrates how to use **tatForge** with **CocoIndex** - the central orchestration framework that wires together:\n- **ColPali** for vision-based multi-vector embeddings (spatial awareness)\n- **Qdrant** for vector storage and semantic search\n- **BAML** for structured LLM extraction from document images\n\n## Architecture\n\n```\nINDEXING (cocoindex setup/update):\n  pdfs/ → LocalFile → file_to_pages → ColPaliEmbedImage → Qdrant\n\nSEARCH + EXTRACT:\n  Query → ColPaliEmbedQuery → Qdrant Search → Page Images\n        → extract_with_baml (cached) → Structured Output\n```\n\n## Prerequisites\n\n| Component | Purpose | Setup |\n|-----------|---------|-------|\n| **CocoIndex** | Flow orchestration | `pip install cocoindex[colpali]` |\n| **Qdrant** | Vector database | Docker: `docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant` |\n| **BAML** | LLM extraction | API key (OpenAI for GPT-4o vision) |\n| **ColPali** | Vision embeddings | Included with `cocoindex[colpali]` |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Check Prerequisites\n\nLet's verify CocoIndex, Qdrant, and BAML are available."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\ndef check_prerequisites():\n    \"\"\"Check all CocoIndex + ColPali prerequisites and report status.\"\"\"\n    status = {}\n    \n    # 1. Check CocoIndex\n    try:\n        import cocoindex\n        status[\"cocoindex\"] = {\"installed\": True, \"message\": \"cocoindex is installed\"}\n    except ImportError:\n        status[\"cocoindex\"] = {\"installed\": False, \"message\": \"Missing: pip install cocoindex[colpali]\"}\n    \n    # 2. Check ColPali engine\n    try:\n        import colpali_engine\n        status[\"colpali\"] = {\"installed\": True, \"message\": \"colpali-engine is installed\"}\n    except ImportError:\n        status[\"colpali\"] = {\"installed\": False, \"message\": \"Missing: pip install cocoindex[colpali]\"}\n    \n    # 3. Check Qdrant client and connection\n    try:\n        from qdrant_client import QdrantClient\n        status[\"qdrant_client\"] = {\"installed\": True, \"message\": \"qdrant-client is installed\"}\n        \n        qdrant_url = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n        try:\n            client = QdrantClient(url=qdrant_url, timeout=5)\n            collections = client.get_collections()\n            status[\"qdrant_server\"] = {\"running\": True, \"message\": f\"Qdrant running at {qdrant_url}\"}\n        except Exception as e:\n            status[\"qdrant_server\"] = {\"running\": False, \"message\": f\"Qdrant not running at {qdrant_url}\"}\n    except ImportError:\n        status[\"qdrant_client\"] = {\"installed\": False, \"message\": \"Missing: pip install qdrant-client\"}\n        status[\"qdrant_server\"] = {\"running\": False, \"message\": \"Install qdrant-client first\"}\n    \n    # 4. Check BAML\n    try:\n        import baml_py\n        status[\"baml\"] = {\"installed\": True, \"message\": \"baml-py is installed\"}\n    except ImportError:\n        status[\"baml\"] = {\"installed\": False, \"message\": \"Missing: pip install baml-py\"}\n    \n    # 5. Check BAML client (generated)\n    try:\n        from baml_client import b\n        status[\"baml_client\"] = {\"installed\": True, \"message\": \"BAML client generated\"}\n    except ImportError:\n        status[\"baml_client\"] = {\"installed\": False, \"message\": \"Run: baml-cli generate\"}\n    \n    # 6. Check for OpenAI API key (required for GPT-4o vision)\n    openai_key = os.getenv(\"OPENAI_API_KEY\")\n    if openai_key:\n        status[\"openai_api\"] = {\"configured\": True, \"message\": f\"OPENAI_API_KEY is set ({openai_key[:8]}...)\"}\n    else:\n        status[\"openai_api\"] = {\"configured\": False, \"message\": \"OPENAI_API_KEY not set (required for GPT-4o vision)\"}\n    \n    # 7. Check tatforge flows module\n    try:\n        sys.path.insert(0, '..')\n        from tatforge.flows import document_indexing_flow, query_to_colpali_embedding\n        status[\"tatforge_flows\"] = {\"installed\": True, \"message\": \"tatforge.flows module available\"}\n    except ImportError as e:\n        status[\"tatforge_flows\"] = {\"installed\": False, \"message\": f\"tatforge.flows not found: {e}\"}\n    \n    return status\n\n# Run checks\nprint(\"=\" * 60)\nprint(\"COCOINDEX + COLPALI PREREQUISITES CHECK\")\nprint(\"=\" * 60)\n\nstatus = check_prerequisites()\n\nall_ready = True\nfor component, info in status.items():\n    ready = info.get(\"installed\", False) or info.get(\"running\", False) or info.get(\"configured\", False)\n    icon = \"✅\" if ready else \"❌\"\n    print(f\"{icon} {component}: {info['message']}\")\n    if not ready:\n        all_ready = False\n\nprint(\"=\" * 60)\nif all_ready:\n    print(\"All prerequisites met! Ready for CocoIndex + ColPali extraction.\")\nelse:\n    print(\"Some prerequisites missing. Follow the setup steps below.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Install CocoIndex (if missing)\n\nCocoIndex is the orchestration framework that manages data flows between BAML and Qdrant."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CocoIndex with ColPali support (uncomment and run if not installed)\n# !pip install cocoindex[colpali]\n\n# Verify installation\ntry:\n    import cocoindex\n    import colpali_engine\n    print(\"✅ CocoIndex with ColPali support installed successfully\")\nexcept ImportError as e:\n    print(f\"❌ Missing dependencies: {e}\")\n    print(\"   Uncomment the pip install line above and run.\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Start Qdrant Vector Database\n\nQdrant stores the ColPali embeddings for semantic search. You can run it via Docker:\n\n```bash\n# In terminal, run:\ndocker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n```\n\nOr use Qdrant Cloud (free tier available at https://cloud.qdrant.io)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check Qdrant connection\nfrom qdrant_client import QdrantClient\nimport os\n\nqdrant_url = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n\ntry:\n    client = QdrantClient(url=qdrant_url, timeout=5)\n    collections = client.get_collections()\n    print(f\"✅ Qdrant is running at {qdrant_url}\")\n    print(f\"   Collections: {len(collections.collections)}\")\nexcept Exception as e:\n    print(f\"❌ Cannot connect to Qdrant at {qdrant_url}\")\n    print(f\"   Error: {e}\")\n    print(f\"\\n   Start Qdrant with: docker run -p 6333:6333 qdrant/qdrant\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 4: Configure LLM API Key\n\nBAML uses an LLM (Claude or GPT-4) for structured extraction. Set your API key:\n\n**Option A**: Set in environment (recommended)\n```bash\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n# or\nexport OPENAI_API_KEY=\"sk-...\"\n```\n\n**Option B**: Set in this notebook (temporary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set API key (uncomment and fill in your key)\nimport os\n\n# Option B: Set key directly (will only persist for this session)\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"  # Your Anthropic key\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Or your OpenAI key\n\n# Check if API key is configured\nanthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\n\nif anthropic_key:\n    print(f\"✅ ANTHROPIC_API_KEY is set ({anthropic_key[:12]}...)\")\nelif openai_key:\n    print(f\"✅ OPENAI_API_KEY is set ({openai_key[:12]}...)\")\nelse:\n    print(\"❌ No LLM API key configured\")\n    print(\"   Set ANTHROPIC_API_KEY or OPENAI_API_KEY above\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Initialize CocoIndex Flow\n\nImport and initialize the CocoIndex flow that orchestrates BAML extraction and Qdrant storage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize CocoIndex and import tatforge flows\nimport os\nimport sys\nimport cocoindex\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add parent directory to path for tatforge imports\nsys.path.insert(0, '..')\n\n# Initialize CocoIndex (REQUIRED before using flows)\ncocoindex.init()\nprint(\"✅ CocoIndex initialized\")\n\n# Import the flow functions from tatforge.flows module\nfrom tatforge.flows import (\n    # Indexing flow (module-level @cocoindex.flow_def)\n    document_indexing_flow,\n    query_to_colpali_embedding,\n    qdrant_connection,\n    # Operations (module-level @cocoindex.op.function)\n    file_to_pages,\n    Page,\n    # Extraction (module-level @cocoindex.op.function with cache)\n    extract_with_baml,\n    extract_with_schema,\n    # Configuration\n    QDRANT_GRPC_URL,\n    QDRANT_COLLECTION,\n    PDF_PATH,\n    COLPALI_MODEL,\n)\n\nprint(f\"✅ tatforge.flows imported\")\nprint(f\"   PDF Path: {PDF_PATH}\")\nprint(f\"   Qdrant gRPC URL: {QDRANT_GRPC_URL}\")\nprint(f\"   Collection: {QDRANT_COLLECTION}\")\nprint(f\"   ColPali Model: {COLPALI_MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The tatforge.flows module defines CocoIndex flows at MODULE LEVEL\n# This is critical - decorators must be at module level, not inside functions\n\nprint(\"tatforge.flows Architecture (from tatforge/flows/):\")\nprint(\"-\" * 60)\nprint(\"\"\"\n# tatforge/flows/ops.py - Operations\n@cocoindex.op.function()\ndef file_to_pages(filename: str, content: bytes) -> list[Page]:\n    '''Convert PDFs to 300 DPI page images'''\n    if mime_type == \"application/pdf\":\n        images = convert_from_bytes(content, dpi=300)\n        return [Page(page_number=i+1, image=buffer.getvalue()) for i, img in enumerate(images)]\n\n# tatforge/flows/indexing.py - ColPali + Qdrant indexing\nqdrant_connection = cocoindex.add_auth_entry(\n    \"qdrant_connection\",\n    cocoindex.targets.QdrantConnection(grpc_url=QDRANT_GRPC_URL),\n)\n\n@cocoindex.flow_def(name=\"DocumentIndexingFlow\")\ndef document_indexing_flow(flow_builder, data_scope) -> None:\n    # Load PDFs as binary\n    data_scope[\"documents\"] = flow_builder.add_source(\n        cocoindex.sources.LocalFile(path=PDF_PATH, binary=True)\n    )\n    \n    with data_scope[\"documents\"].row() as doc:\n        # Convert to page images\n        doc[\"pages\"] = flow_builder.transform(file_to_pages, ...)\n        \n        with doc[\"pages\"].row() as page:\n            # Generate ColPali embedding (multi-vector for spatial awareness)\n            page[\"embedding\"] = page[\"image\"].transform(\n                cocoindex.functions.ColPaliEmbedImage(model=COLPALI_MODEL)\n            )\n            output_embeddings.collect(...)\n    \n    # Export to Qdrant\n    output_embeddings.export(\"document_embeddings\", cocoindex.targets.Qdrant(...))\n\n@cocoindex.transform_flow()\ndef query_to_colpali_embedding(text):\n    '''Convert query to ColPali multi-vector embedding'''\n    return text.transform(cocoindex.functions.ColPaliEmbedQuery(model=COLPALI_MODEL))\n\n# tatforge/flows/extraction.py - BAML extraction with caching\n@cocoindex.op.function(cache=True, behavior_version=1)\nasync def extract_with_baml(page_image: bytes, extraction_prompt: str) -> dict:\n    '''Extract structured data from page image using BAML (cached)'''\n    image = baml_py.Image.from_base64(\"image/png\", base64.b64encode(page_image))\n    return await b.ExtractDocumentFieldsFromImage(document_image=image, ...)\n\"\"\")\nprint(\"-\" * 60)\nprint(\"✅ Flows defined at module level. Use CLI to run:\")\nprint(\"   cocoindex setup  - Create Qdrant collection with schema\")\nprint(\"   cocoindex update - Index all PDFs with ColPali embeddings\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Direct BAML Extraction (from page images)\n\nColPali works with **page images**, not raw PDFs. The `file_to_pages` operation converts PDFs to 300 DPI PNG images. You can test BAML extraction directly on these images."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert PDF to page images using tatforge.flows.file_to_pages\nfrom pathlib import Path\n\n# Select a PDF to process\npdf_path = Path(\"../pdfs/Bunge_loadingstatement_2025-09-25.pdf\")\n\nif not pdf_path.exists():\n    # Try alternative PDFs\n    pdf_dir = Path(\"../pdfs\")\n    pdfs = list(pdf_dir.glob(\"*.pdf\"))\n    if pdfs:\n        pdf_path = pdfs[0]\n        print(f\"Using available PDF: {pdf_path.name}\")\n    else:\n        print(\"❌ No PDFs found in ../pdfs/ directory\")\n        pdf_path = None\n\nif pdf_path:\n    print(f\"Processing: {pdf_path.name}\")\n    \n    # Read PDF bytes\n    pdf_bytes = pdf_path.read_bytes()\n    print(f\"PDF size: {len(pdf_bytes):,} bytes\")\n    \n    # Convert to page images using tatforge operation\n    pages = file_to_pages(pdf_path.name, pdf_bytes)\n    print(f\"Converted to {len(pages)} page image(s)\")\n    \n    if pages:\n        # Get first page for extraction demo\n        first_page = pages[0]\n        page_image = first_page.image\n        print(f\"First page image size: {len(page_image):,} bytes (PNG @ 300 DPI)\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 7: Run BAML Extraction\n\nCall the BAML extraction function with the PDF document."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run BAML extraction on the page image\nextraction_prompt = \"\"\"\nExtract all shipping information from this loading statement document.\nReturn as JSON with the following structure:\n{\n    \"document_title\": \"title of the document\",\n    \"last_updated\": \"date and author of last update\",\n    \"shipments\": [\n        {\n            \"slot_reference\": \"unique reference number\",\n            \"vessel_name\": \"name of the ship\",\n            \"port\": \"port name\",\n            \"eta_from\": \"ETA from date\",\n            \"eta_to\": \"ETA to date\",\n            \"commodity\": \"type of cargo\",\n            \"quantity_tonnes\": \"quantity in tonnes\",\n            \"exporter\": \"exporter name\",\n            \"loading_status\": \"status of loading\"\n        }\n    ]\n}\n\"\"\"\n\nif 'page_image' in dir():\n    print(\"Running BAML extraction with GPT-4o vision on page image...\")\n    print(\"-\" * 60)\n    \n    # Use the cached extract_with_baml function from tatforge.flows\n    result = await extract_with_baml(page_image, extraction_prompt)\n    \n    print(\"✅ Extraction complete!\")\n    print(\"-\" * 60)\nelse:\n    print(\"❌ No page_image available. Run the previous cell first.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display extracted data\nimport json\n\nprint(\"Extracted Data:\")\nprint(\"=\" * 60)\n\nif 'result' in dir():\n    # extract_with_baml returns a dict with status and extracted_text or error\n    if result.get(\"status\") == \"success\":\n        print(f\"Status: {result['status']}\")\n        print(\"-\" * 60)\n        extracted = result.get(\"extracted_text\", \"\")\n        # Try to pretty-print if it's JSON\n        try:\n            if isinstance(extracted, str):\n                parsed = json.loads(extracted)\n                print(json.dumps(parsed, indent=2))\n            else:\n                print(json.dumps(extracted, indent=2))\n        except (json.JSONDecodeError, TypeError):\n            print(extracted)\n    else:\n        print(f\"Status: {result.get('status', 'unknown')}\")\n        if \"error\" in result:\n            print(f\"Error: {result['error']}\")\n        if \"message\" in result:\n            print(f\"Message: {result['message']}\")\nelse:\n    print(\"❌ No result available. Run the extraction cell first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Semantic Search with ColPali\n\nAfter running `cocoindex setup` and `cocoindex update`, you can search documents using ColPali's multi-vector embeddings. This provides spatial awareness - ColPali understands document layout and visual structure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search documents using ColPali embeddings\n# (Requires running 'cocoindex setup' and 'cocoindex update' first)\n\nfrom qdrant_client import QdrantClient\n\nquery = \"shipping vessel wheat loading\"\nprint(f\"Searching for: '{query}'\")\nprint(\"-\" * 60)\n\ntry:\n    # Generate ColPali query embedding (multi-vector)\n    query_embedding = query_to_colpali_embedding.eval(query)\n    print(f\"Query embedding shape: {len(query_embedding)} vectors\")\n    \n    # Search Qdrant with multi-vector similarity\n    client = QdrantClient(url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"))\n    \n    # Check if collection exists\n    collections = [c.name for c in client.get_collections().collections]\n    if QDRANT_COLLECTION not in collections:\n        print(f\"\\n❌ Collection '{QDRANT_COLLECTION}' not found.\")\n        print(\"   Run these commands first:\")\n        print(\"   1. cocoindex setup   (creates collection)\")\n        print(\"   2. cocoindex update  (indexes documents)\")\n    else:\n        # Perform search with ColPali multi-vector\n        results = client.query_points(\n            collection_name=QDRANT_COLLECTION,\n            query=query_embedding,\n            limit=5,\n            with_payload=True,\n        )\n        \n        print(f\"\\nFound {len(results.points)} results:\")\n        for i, point in enumerate(results.points, 1):\n            payload = point.payload or {}\n            filename = payload.get(\"filename\", \"unknown\")\n            page = payload.get(\"page\", \"?\")\n            score = point.score\n            print(f\"\\n{i}. [{score:.3f}] {filename} (page {page})\")\n            \nexcept Exception as e:\n    print(f\"Search error: {e}\")\n    print(\"\\nMake sure you've run:\")\n    print(\"  1. cocoindex setup   - Create Qdrant collection\")\n    print(\"  2. cocoindex update  - Index documents with ColPali\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've completed the tatForge + CocoIndex + ColPali quickstart! You learned how to:\n\n1. **Check prerequisites** - CocoIndex, ColPali, Qdrant, BAML, OpenAI API key\n2. **Initialize CocoIndex** - Central orchestration framework with `cocoindex.init()`\n3. **Import tatforge.flows** - Module-level flow definitions for proper CocoIndex integration\n4. **Convert PDFs to images** - Using `file_to_pages` operation (300 DPI PNG)\n5. **BAML extraction** - Using `extract_with_baml` with caching for efficient LLM calls\n6. **ColPali search** - Multi-vector embeddings for spatial-aware document retrieval\n\n## Key Architecture Points\n\n| Component | Role | CocoIndex Integration |\n|-----------|------|----------------------|\n| **ColPali** | Vision embeddings | `cocoindex.functions.ColPaliEmbedImage/Query()` |\n| **Qdrant** | Vector storage | `cocoindex.targets.Qdrant()` with gRPC |\n| **BAML** | Structured extraction | `@cocoindex.op.function(cache=True)` |\n| **pdf2image** | PDF → PNG conversion | `@cocoindex.op.function()` |\n\n## Commands\n\n```bash\n# Setup Qdrant collection with proper schema\ncocoindex setup\n\n# Index all PDFs with ColPali embeddings\ncocoindex update\n\n# Or use tatforge CLI\ntatforge cocoindex setup\ntatforge cocoindex update\n\n# Or run the main entry point\npython main.py\n```\n\n## File Structure\n\n```\ntatforge/flows/\n├── __init__.py      # Exports all flows and operations\n├── ops.py           # file_to_pages operation\n├── indexing.py      # document_indexing_flow, query_to_colpali_embedding\n└── extraction.py    # extract_with_baml, extract_with_schema (cached)\n```\n\n## Next Steps\n\n- Add more PDFs to the `pdfs/` directory\n- Run `cocoindex update` to index new documents\n- Customize extraction prompts in your code\n- Create custom BAML schemas in `baml_src/`\n- Read the [CocoIndex docs](https://cocoindex.io/docs)\n\n## Support\n\nFor issues: https://github.com/Frosselet/COCOINDEX_LEARNING/issues"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}