{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# tatForge Quick Start Guide\n\nThis notebook demonstrates how to use **tatForge** - an AI-powered document extraction library that uses vision AI to forge structured data from unstructured documents.\n\n## Prerequisites\n\nBefore using tatforge, you need to set up three components:\n\n| Component | Purpose | Setup |\n|-----------|---------|-------|\n| **ColPali** | Vision embeddings (3B model) | `pip install colpali-engine` |\n| **Qdrant** | Vector database | Docker container |\n| **BAML** | LLM extraction | API key (Anthropic/OpenAI) |\n\nThis notebook will guide you through setting up each prerequisite."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Check Prerequisites\n\nLet's verify which components are available and which need to be installed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\ndef check_prerequisites():\n    \"\"\"Check all tatforge prerequisites and report status.\"\"\"\n    status = {}\n    \n    # 1. Check ColPali engine\n    try:\n        from colpali_engine.models import ColPali, ColPaliProcessor\n        status[\"colpali\"] = {\"installed\": True, \"message\": \"colpali-engine is installed\"}\n    except ImportError as e:\n        status[\"colpali\"] = {\"installed\": False, \"message\": f\"Missing: pip install colpali-engine\"}\n    \n    # 2. Check Qdrant client and connection\n    try:\n        from qdrant_client import QdrantClient\n        status[\"qdrant_client\"] = {\"installed\": True, \"message\": \"qdrant-client is installed\"}\n        \n        # Try to connect to Qdrant\n        qdrant_url = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n        try:\n            client = QdrantClient(url=qdrant_url, timeout=5)\n            collections = client.get_collections()\n            status[\"qdrant_server\"] = {\"running\": True, \"message\": f\"Qdrant running at {qdrant_url}\"}\n        except Exception as e:\n            status[\"qdrant_server\"] = {\"running\": False, \"message\": f\"Qdrant not running at {qdrant_url}\"}\n    except ImportError:\n        status[\"qdrant_client\"] = {\"installed\": False, \"message\": \"Missing: pip install qdrant-client\"}\n        status[\"qdrant_server\"] = {\"running\": False, \"message\": \"Install qdrant-client first\"}\n    \n    # 3. Check BAML and API keys\n    try:\n        import baml_py\n        status[\"baml\"] = {\"installed\": True, \"message\": f\"baml-py is installed\"}\n    except ImportError:\n        status[\"baml\"] = {\"installed\": False, \"message\": \"Missing: pip install baml-py\"}\n    \n    # Check for LLM API keys\n    anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    openai_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    if anthropic_key:\n        status[\"llm_api\"] = {\"configured\": True, \"message\": \"ANTHROPIC_API_KEY is set\"}\n    elif openai_key:\n        status[\"llm_api\"] = {\"configured\": True, \"message\": \"OPENAI_API_KEY is set\"}\n    else:\n        status[\"llm_api\"] = {\"configured\": False, \"message\": \"No LLM API key found (ANTHROPIC_API_KEY or OPENAI_API_KEY)\"}\n    \n    # 4. Check tatforge\n    try:\n        import tatforge\n        status[\"tatforge\"] = {\"installed\": True, \"message\": f\"tatforge {tatforge.__version__} is installed\"}\n    except ImportError:\n        status[\"tatforge\"] = {\"installed\": False, \"message\": \"Missing: pip install -e .\"}\n    \n    return status\n\n# Run checks\nprint(\"=\" * 60)\nprint(\"TATFORGE PREREQUISITES CHECK\")\nprint(\"=\" * 60)\n\nstatus = check_prerequisites()\n\nall_ready = True\nfor component, info in status.items():\n    ready = info.get(\"installed\", False) or info.get(\"running\", False) or info.get(\"configured\", False)\n    icon = \"‚úÖ\" if ready else \"‚ùå\"\n    print(f\"{icon} {component}: {info['message']}\")\n    if not ready:\n        all_ready = False\n\nprint(\"=\" * 60)\nif all_ready:\n    print(\"All prerequisites met! You can proceed with extraction.\")\nelse:\n    print(\"Some prerequisites missing. Follow the setup steps below.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Install ColPali Engine (if missing)\n\nColPali is a vision-language model that generates patch-level embeddings from document images. The model is ~3B parameters and will be downloaded from HuggingFace."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install ColPali engine (uncomment and run if not installed)\n# This will download the model from HuggingFace (~6GB)\n\n# !pip install colpali-engine\n\n# Verify installation\ntry:\n    from colpali_engine.models import ColPali, ColPaliProcessor\n    print(\"‚úÖ ColPali engine installed successfully\")\nexcept ImportError:\n    print(\"‚ùå ColPali not installed. Uncomment the pip install line above and run.\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Start Qdrant Vector Database\n\nQdrant stores the ColPali embeddings for semantic search. You can run it via Docker:\n\n```bash\n# In terminal, run:\ndocker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n```\n\nOr use Qdrant Cloud (free tier available at https://cloud.qdrant.io)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check Qdrant connection\nfrom qdrant_client import QdrantClient\nimport os\n\nqdrant_url = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n\ntry:\n    client = QdrantClient(url=qdrant_url, timeout=5)\n    collections = client.get_collections()\n    print(f\"‚úÖ Qdrant is running at {qdrant_url}\")\n    print(f\"   Collections: {len(collections.collections)}\")\nexcept Exception as e:\n    print(f\"‚ùå Cannot connect to Qdrant at {qdrant_url}\")\n    print(f\"   Error: {e}\")\n    print(f\"\\n   Start Qdrant with: docker run -p 6333:6333 qdrant/qdrant\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 4: Configure LLM API Key\n\nBAML uses an LLM (Claude or GPT-4) for structured extraction. Set your API key:\n\n**Option A**: Set in environment (recommended)\n```bash\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n# or\nexport OPENAI_API_KEY=\"sk-...\"\n```\n\n**Option B**: Set in this notebook (temporary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set API key (uncomment and fill in your key)\nimport os\n\n# Option B: Set key directly (will only persist for this session)\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"  # Your Anthropic key\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Or your OpenAI key\n\n# Check if API key is configured\nanthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\n\nif anthropic_key:\n    print(f\"‚úÖ ANTHROPIC_API_KEY is set ({anthropic_key[:12]}...)\")\nelif openai_key:\n    print(f\"‚úÖ OPENAI_API_KEY is set ({openai_key[:12]}...)\")\nelse:\n    print(\"‚ùå No LLM API key configured\")\n    print(\"   Set ANTHROPIC_API_KEY or OPENAI_API_KEY above\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Initialize tatforge Components\n\nOnce all prerequisites are met, initialize the tatforge pipeline components."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize tatforge components\nfrom tatforge.vision.colpali_client import ColPaliClient\nfrom tatforge.storage.qdrant_client import QdrantManager\nfrom tatforge.extraction.baml_interface import BAMLExecutionInterface\nfrom tatforge.outputs.canonical import CanonicalFormatter\n\n# Initialize ColPali client (lazy loading - model loads on first use)\ncolpali_client = ColPaliClient(\n    model_name=\"vidore/colqwen2-v0.1\",\n    device=\"auto\",  # Will use CUDA if available, else CPU\n    memory_limit_gb=8\n)\nprint(\"‚úÖ ColPali client initialized (model will load on first use)\")\n\n# Initialize Qdrant manager\nqdrant_manager = QdrantManager(\n    url=os.getenv(\"QDRANT_URL\", \"http://localhost:6333\"),\n    collection_name=\"tatforge_embeddings\"\n)\nprint(\"‚úÖ Qdrant manager initialized\")\n\n# Initialize BAML execution interface\nbaml_interface = BAMLExecutionInterface(\n    colpali_client=colpali_client,\n    qdrant_manager=qdrant_manager\n)\nprint(\"‚úÖ BAML interface initialized\")\n\n# Initialize canonical formatter\ncanonical_formatter = CanonicalFormatter()\nprint(\"‚úÖ Canonical formatter initialized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Connect to services and load models\n# This may take a few minutes on first run as it downloads the ColPali model\n\nimport asyncio\n\nasync def initialize_services():\n    \"\"\"Initialize all tatforge services.\"\"\"\n    # Connect to Qdrant\n    print(\"Connecting to Qdrant...\")\n    await qdrant_manager.connect()\n    await qdrant_manager.ensure_collection()\n    print(\"‚úÖ Qdrant connected and collection ready\")\n    \n    # Load ColPali model (this downloads ~6GB on first run)\n    print(\"Loading ColPali model (this may take a few minutes on first run)...\")\n    await colpali_client.load_model()\n    print(\"‚úÖ ColPali model loaded\")\n    \n    print(\"\\nüéâ All services initialized! Ready for extraction.\")\n\n# Run initialization\nawait initialize_services()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Define Extraction Schema\n\nDefine a JSON schema for the data you want to extract.\n\n### Allowed Schema Types\n\n| Type | Description | Example |\n|------|-------------|---------|\n| `string` | Text values | `\"type\": \"string\"` |\n| `int` | Integer numbers | `\"type\": \"int\"` |\n| `float` | Decimal numbers | `\"type\": \"float\"` |\n| `bool` | Boolean true/false | `\"type\": \"bool\"` |\n| `array` | List of items | `\"type\": \"array\", \"items\": {...}` |\n| `object` | Nested structure | `\"type\": \"object\", \"properties\": {...}` |\n\n**Note**: Use `int` and `float` (not `integer` or `number`) for BAML compatibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define extraction schema for Bunge Loading Statement\nloading_statement_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"document_title\": {\n            \"type\": \"string\",\n            \"description\": \"Title or type of document\"\n        },\n        \"last_updated\": {\n            \"type\": \"string\",\n            \"description\": \"Last updated date and author\"\n        },\n        \"shipments\": {\n            \"type\": \"array\",\n            \"description\": \"List of shipping entries\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"slot_reference\": {\"type\": \"string\", \"description\": \"Unique slot reference number (e.g., BG20250025)\"},\n                    \"vessel_name\": {\"type\": \"string\", \"description\": \"Name of ship\"},\n                    \"nomination_received_date\": {\"type\": \"string\", \"description\": \"Date nomination was received\"},\n                    \"nomination_accepted_date\": {\"type\": \"string\", \"description\": \"Date nomination was accepted\"},\n                    \"port\": {\"type\": \"string\", \"description\": \"Port name\"},\n                    \"eta_from_date\": {\"type\": \"string\", \"description\": \"ETA of ship from date\"},\n                    \"eta_to_date\": {\"type\": \"string\", \"description\": \"ETA of ship to date\"},\n                    \"loading_commencement_date\": {\"type\": \"string\", \"description\": \"ETA of grain loading commencement date\"},\n                    \"etd_date\": {\"type\": \"string\", \"description\": \"ETD of ship date\"},\n                    \"exporter\": {\"type\": \"string\", \"description\": \"Exporter name\"},\n                    \"quantity_tonnes\": {\"type\": \"int\", \"description\": \"Quantity in tonnes\"},\n                    \"commodity\": {\"type\": \"string\", \"description\": \"Commodity type (WHEAT, BARLEY, CANOLA, MALT)\"},\n                    \"loading_status\": {\"type\": \"string\", \"description\": \"Loading status: COMMENCED, COMPLETED, or empty\"},\n                    \"loading_completed_date\": {\"type\": \"string\", \"description\": \"Date loading completed\"},\n                    \"notes\": {\"type\": \"string\", \"description\": \"Additional notes\"}\n                }\n            }\n        }\n    }\n}\n\nprint(f\"Schema defined with {len(loading_statement_schema['properties'])} top-level fields\")\nprint(f\"Shipment record has {len(loading_statement_schema['properties']['shipments']['items']['properties'])} fields\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 7: Extract Data from Document\n\nNow run the full extraction pipeline on a PDF document."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Extract data from the Bunge loading statement PDF\nfrom pathlib import Path\nfrom tatforge import PDFAdapter\nfrom tatforge.core.pipeline import VisionExtractionPipeline, PipelineConfig\n\n# Path to PDF\npdf_path = Path(\"../pdfs/Bunge_loadingstatement_2025-09-25.pdf\")\nprint(f\"Processing: {pdf_path.name}\")\n\n# Convert PDF to image frames\npdf_adapter = PDFAdapter(max_memory_mb=500)\nframes = await pdf_adapter.convert_to_frames(pdf_path)\nprint(f\"Converted PDF to {len(frames)} image frame(s)\")\n\n# Create and run the pipeline\nconfig = PipelineConfig(\n    memory_limit_gb=8,\n    batch_size=\"auto\",\n    enable_shaped_output=True\n)\n\npipeline = VisionExtractionPipeline(\n    colpali_client=colpali_client,\n    qdrant_manager=qdrant_manager,\n    baml_interface=baml_interface,\n    canonical_formatter=canonical_formatter,\n    config=config\n)\n\n# Process the document\nprint(\"Running extraction pipeline...\")\nwith open(pdf_path, \"rb\") as f:\n    document_blob = f.read()\n\nresult = await pipeline.process_document(\n    document_blob=document_blob,\n    schema_json=loading_statement_schema\n)\n\nprint(f\"\\n‚úÖ Extraction completed!\")\nprint(f\"   Processing ID: {result.metadata.processing_id}\")\nprint(f\"   Processing time: {result.metadata.processing_time_seconds:.2f}s\")\nprint(f\"   Status: {result.metadata.status}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display extracted data\nimport json\n\nif result.canonical and result.canonical.extraction_data:\n    print(\"Extracted Data:\")\n    print(\"-\" * 60)\n    print(json.dumps(result.canonical.extraction_data, indent=2))\nelse:\n    print(\"No data extracted. Check the pipeline logs above for details.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Export Results\n\nExport extracted data to various formats."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export to different formats\nfrom tatforge import DataExporter\nfrom pathlib import Path\n\nexporter = DataExporter()\n\nif result.canonical and result.canonical.extraction_data:\n    output_dir = Path(\"../outputs\")\n    output_dir.mkdir(exist_ok=True)\n    \n    # Export to JSON\n    json_path = output_dir / \"bunge_extraction.json\"\n    exporter.export_json(result.canonical.extraction_data, json_path)\n    print(f\"‚úÖ Exported to JSON: {json_path}\")\n    \n    # Export to CSV (if shipments array exists)\n    if \"shipments\" in result.canonical.extraction_data:\n        csv_path = output_dir / \"bunge_shipments.csv\"\n        exporter.export_csv(result.canonical.extraction_data[\"shipments\"], csv_path)\n        print(f\"‚úÖ Exported to CSV: {csv_path}\")\nelse:\n    print(\"No data to export\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've completed the tatforge quickstart! You learned how to:\n\n1. **Check prerequisites** - ColPali, Qdrant, BAML, API keys\n2. **Install dependencies** - colpali-engine package\n3. **Start services** - Qdrant vector database\n4. **Configure API keys** - For LLM extraction\n5. **Initialize components** - ColPali client, Qdrant manager, BAML interface\n6. **Define schemas** - JSON schema with BAML-compatible types\n7. **Extract data** - Run the full extraction pipeline\n8. **Export results** - JSON, CSV, Parquet formats\n\n## Next Steps\n\n- Try different PDFs from the `pdfs/` directory\n- Create custom schemas for your documents\n- Explore the CLI: `tatforge --help`\n- Read the [Architecture Playbook](../docs/architecture-playbook.md)\n\n## Support\n\nFor issues or questions: https://github.com/Frosselet/COCOINDEX_LEARNING/issues"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}