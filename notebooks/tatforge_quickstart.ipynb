{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tatForge Shipping Document Extraction\n",
    "\n",
    "Extract structured data from shipping stem documents using **GPT-4o vision** via BAML.\n",
    "\n",
    "## Document Types Covered\n",
    "\n",
    "| Document | Structure | Challenge |\n",
    "|----------|-----------|----------|\n",
    "| **CBH Shipping Stem** | Multi-port tables with merged header cells | Port name in colored header |\n",
    "| **Queensland Bulk Terminals** | Vertical label-value layout | Multiple columns per page |\n",
    "| **GrainCorp Shipping Stem** | Very dense tabular data | High density, many rows skipped |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import dependencies\n",
    "import baml_py\n",
    "from baml_client import b\n",
    "from tatforge.flows import file_to_pages\n",
    "\n",
    "# Verify setup\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"OPENAI_API_KEY: {'Set' if api_key else 'NOT SET'}\")\n",
    "print(f\"BAML client: {hasattr(b, 'ExtractDocumentFieldsFromImage')}\")\n",
    "\n",
    "# Helper function for extraction\n",
    "def extract_from_pdf(pdf_path: Path, prompt: str, page_num: int = 0):\n",
    "    \"\"\"Extract data from a specific page of a PDF.\"\"\"\n",
    "    pdf_bytes = pdf_path.read_bytes()\n",
    "    pages = file_to_pages(pdf_path.name, pdf_bytes)\n",
    "    \n",
    "    if page_num >= len(pages):\n",
    "        raise ValueError(f\"Page {page_num} not found. PDF has {len(pages)} pages.\")\n",
    "    \n",
    "    page_image = pages[page_num].image\n",
    "    image_b64 = base64.b64encode(page_image).decode(\"utf-8\")\n",
    "    baml_image = baml_py.Image.from_base64(\"image/png\", image_b64)\n",
    "    \n",
    "    result = b.ExtractDocumentFieldsFromImage(\n",
    "        document_image=baml_image,\n",
    "        extraction_prompt=prompt\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(result)\n",
    "    except json.JSONDecodeError:\n",
    "        return result\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: CBH Shipping Stem\n",
    "\n",
    "**File:** `CBH Shipping Stem 26092025.pdf`\n",
    "\n",
    "**Structure:** Multi-port document with tables grouped by port. Port name appears in a colored merged header cell above each table section.\n",
    "\n",
    "**Challenge:** The port name (GERALDTON, KWINANA, ALBANY, ESPERANCE) is in the first merged cell of each section, not in the data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBH Shipping Stem - with port name in schema\n",
    "cbh_pdf = Path(\"../pdfs/CBH Shipping Stem 26092025.pdf\")\n",
    "\n",
    "cbh_prompt = \"\"\"\n",
    "This is a CBH Daily Ship Roster document with multiple port sections.\n",
    "\n",
    "IMPORTANT: Each section has a PORT NAME in a colored header row (e.g., \"GERALDTON\", \"KWINANA\", \"ALBANY\", \"ESPERANCE\").\n",
    "You MUST include the port name for each shipment.\n",
    "\n",
    "Extract ALL shipments from ALL port sections. Return as JSON:\n",
    "\n",
    "{\n",
    "    \"document_date\": \"date from 'As of' field\",\n",
    "    \"shipments\": [\n",
    "        {\n",
    "            \"port\": \"port name from the colored section header (GERALDTON/KWINANA/ALBANY/ESPERANCE)\",\n",
    "            \"vna_number\": \"VNA # column\",\n",
    "            \"vessel_name\": \"Vessel Name\",\n",
    "            \"client\": \"Client/Exporter name\",\n",
    "            \"eta_date\": \"ETA date\",\n",
    "            \"etc_date\": \"ETC (Estimated Time of Commencement) date\",\n",
    "            \"etd_date\": \"ETD (Estimated Time of Departure) date\",\n",
    "            \"volume_tonnes\": \"Volume in tonnes (number only)\",\n",
    "            \"commodity\": \"Wheat/Barley/Canola/Lupins etc\",\n",
    "            \"loading_status\": \"Completed/Commenced/blank\",\n",
    "            \"other_ports\": \"if vessel visits other ports (e.g., 'ex ALBANY')\"\n",
    "        }\n",
    "    ],\n",
    "    \"port_totals\": {\n",
    "        \"GERALDTON\": \"total tonnes\",\n",
    "        \"KWINANA\": \"total tonnes\",\n",
    "        \"ALBANY\": \"total tonnes\",\n",
    "        \"ESPERANCE\": \"total tonnes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "CRITICAL: Do not skip any rows. Extract EVERY shipment from EVERY port section.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Processing: {cbh_pdf.name}\")\n",
    "print(\"Extracting...\")\n",
    "\n",
    "cbh_result = extract_from_pdf(cbh_pdf, cbh_prompt, page_num=0)\n",
    "\n",
    "print(\"\\nExtracted Data:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(cbh_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate CBH extraction - count shipments by port\n",
    "if isinstance(cbh_result, dict) and \"shipments\" in cbh_result:\n",
    "    shipments = cbh_result[\"shipments\"]\n",
    "    print(f\"Total shipments extracted: {len(shipments)}\")\n",
    "    \n",
    "    # Count by port\n",
    "    port_counts = {}\n",
    "    for s in shipments:\n",
    "        port = s.get(\"port\", \"UNKNOWN\")\n",
    "        port_counts[port] = port_counts.get(port, 0) + 1\n",
    "    \n",
    "    print(\"\\nShipments by port:\")\n",
    "    for port, count in sorted(port_counts.items()):\n",
    "        print(f\"  {port}: {count}\")\n",
    "    \n",
    "    # Check for missing ports\n",
    "    expected_ports = {\"GERALDTON\", \"KWINANA\", \"ALBANY\", \"ESPERANCE\"}\n",
    "    found_ports = set(port_counts.keys())\n",
    "    missing = expected_ports - found_ports\n",
    "    if missing:\n",
    "        print(f\"\\nWARNING: Missing ports: {missing}\")\n",
    "else:\n",
    "    print(\"Extraction did not return expected structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: Queensland Bulk Terminals\n",
    "\n",
    "**File:** `document (1).pdf`\n",
    "\n",
    "**Structure:** Vertical label-value format with 2 shipment columns per page. Multiple pages.\n",
    "\n",
    "**Challenge:** Data is arranged vertically (field names on left, values in columns), not in typical table rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queensland Bulk Terminals\n",
    "qbt_pdf = Path(\"../pdfs/document (1).pdf\")\n",
    "\n",
    "qbt_prompt = \"\"\"\n",
    "This is a Queensland Bulk Terminals shipping document with a VERTICAL layout.\n",
    "\n",
    "The document shows 2 shipments per page in columns. Field labels are on the left,\n",
    "and values are in columns to the right.\n",
    "\n",
    "Extract ALL shipments from this page. Return as JSON:\n",
    "\n",
    "{\n",
    "    \"last_updated\": \"from 'Last updated on' field\",\n",
    "    \"shipments\": [\n",
    "        {\n",
    "            \"slot_reference\": \"Unique Slot Reference Number\",\n",
    "            \"vessel_name\": \"Name of ship\",\n",
    "            \"port\": \"Port name (e.g., Brisbane)\",\n",
    "            \"nomination_date\": \"Date at which nomination was received\",\n",
    "            \"nomination_time\": \"Time at which nomination was received\",\n",
    "            \"acceptance_date\": \"Date at which nomination was accepted\",\n",
    "            \"acceptance_time\": \"Time at which nomination was accepted\",\n",
    "            \"eta_from_date\": \"Date of ETA of Ship From\",\n",
    "            \"eta_to_date\": \"Date of ETA of Ship To\",\n",
    "            \"loading_commencement_date\": \"Date ETA of Grain Loading Commencement\",\n",
    "            \"etd_date\": \"Date of ETD of Ship\",\n",
    "            \"exporter\": \"Exporter name\",\n",
    "            \"quantity_tonnes\": \"Quantity in tonnes (number only)\",\n",
    "            \"commodity\": \"Commodity type (e.g., QBT Wheat, QBT Sorghum)\",\n",
    "            \"loading_status\": \"Loading commenced or completed\",\n",
    "            \"loading_completed_date\": \"Date Loading Completed (if applicable)\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "Extract BOTH columns on this page (2 shipments).\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Processing: {qbt_pdf.name}\")\n",
    "\n",
    "# Extract from first page\n",
    "print(\"\\nPage 1:\")\n",
    "qbt_page1 = extract_from_pdf(qbt_pdf, qbt_prompt, page_num=0)\n",
    "print(json.dumps(qbt_page1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ALL pages from QBT document\n",
    "qbt_bytes = qbt_pdf.read_bytes()\n",
    "qbt_pages = file_to_pages(qbt_pdf.name, qbt_bytes)\n",
    "\n",
    "print(f\"Total pages: {len(qbt_pages)}\")\n",
    "\n",
    "all_qbt_shipments = []\n",
    "\n",
    "for i in range(len(qbt_pages)):\n",
    "    print(f\"Processing page {i+1}/{len(qbt_pages)}...\")\n",
    "    try:\n",
    "        result = extract_from_pdf(qbt_pdf, qbt_prompt, page_num=i)\n",
    "        if isinstance(result, dict) and \"shipments\" in result:\n",
    "            for s in result[\"shipments\"]:\n",
    "                s[\"source_page\"] = i + 1\n",
    "                all_qbt_shipments.append(s)\n",
    "            print(f\"  Found {len(result['shipments'])} shipments\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "print(f\"\\nTotal QBT shipments extracted: {len(all_qbt_shipments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: GrainCorp Shipping Stem (Dense Document)\n",
    "\n",
    "**File:** `shipping-stem-2025-11-13.pdf`\n",
    "\n",
    "**Structure:** Very dense tabular data organized by Month and Port. Many columns, many rows.\n",
    "\n",
    "**Challenge:** Document is extremely dense. Simple extraction skips many rows. Need strategies:\n",
    "1. Extract by port section\n",
    "2. Use more specific prompts\n",
    "3. Process page by page with row counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrainCorp Shipping Stem - Dense document\n",
    "graincorp_pdf = Path(\"../pdfs/shipping-stem-2025-11-13.pdf\")\n",
    "\n",
    "# Strategy 1: Simple extraction (baseline - expect missing rows)\n",
    "graincorp_simple_prompt = \"\"\"\n",
    "This is a GrainCorp Shipping Stem document with dense tabular data.\n",
    "\n",
    "Extract ALL shipments. Return as JSON:\n",
    "\n",
    "{\n",
    "    \"document_date\": \"Thursday, 13 November 2025\",\n",
    "    \"shipments\": [\n",
    "        {\n",
    "            \"month\": \"Month (e.g., Nov 25, Dec 25)\",\n",
    "            \"port\": \"Port name\",\n",
    "            \"slot_reference\": \"Unique Slot Reference Number\",\n",
    "            \"exporter\": \"Exporter code (GCOP, BUNGE, CARG, etc.)\",\n",
    "            \"vessel_name\": \"Name Of Ship\",\n",
    "            \"eta_date\": \"Date ETA of Ship\",\n",
    "            \"loading_commencement_date\": \"Date of Grain Loading Commencement\",\n",
    "            \"etd_date\": \"Date ETD of Ship\",\n",
    "            \"status\": \"Status (Accepted/COMMENCED)\",\n",
    "            \"commodity\": \"Commodity type\",\n",
    "            \"total_tonnes\": \"Total tonnes (number only)\"\n",
    "        }\n",
    "    ],\n",
    "    \"grand_total\": \"Grand Total tonnes\"\n",
    "}\n",
    "\n",
    "IMPORTANT: This document is VERY DENSE. Count every row carefully.\n",
    "Do NOT skip any rows. Extract EVERY single shipment.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Processing: {graincorp_pdf.name}\")\n",
    "print(\"\\nStrategy 1: Simple extraction (Page 1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "graincorp_simple = extract_from_pdf(graincorp_pdf, graincorp_simple_prompt, page_num=0)\n",
    "\n",
    "if isinstance(graincorp_simple, dict) and \"shipments\" in graincorp_simple:\n",
    "    print(f\"Shipments found: {len(graincorp_simple['shipments'])}\")\n",
    "    print(f\"Grand total: {graincorp_simple.get('grand_total', 'N/A')}\")\n",
    "else:\n",
    "    print(\"Unexpected result format\")\n",
    "    print(json.dumps(graincorp_simple, indent=2)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Extract by PORT SECTION for better accuracy\n",
    "graincorp_port_prompt = \"\"\"\n",
    "This is a GrainCorp Shipping Stem. Extract shipments for ONE PORT SECTION at a time.\n",
    "\n",
    "Focus on the \"{port}\" section only. Return as JSON:\n",
    "\n",
    "{{\n",
    "    \"port\": \"{port}\",\n",
    "    \"shipments\": [\n",
    "        {{\n",
    "            \"month\": \"Month\",\n",
    "            \"slot_reference\": \"Reference number\",\n",
    "            \"exporter\": \"Exporter code\",\n",
    "            \"vessel_name\": \"Ship name\",\n",
    "            \"eta_date\": \"ETA date\",\n",
    "            \"etd_date\": \"ETD date\",\n",
    "            \"commodity\": \"Commodity\",\n",
    "            \"total_tonnes\": \"Tonnes\"\n",
    "        }}\n",
    "    ],\n",
    "    \"port_total\": \"Total for this port section\"\n",
    "}}\n",
    "\n",
    "Extract EVERY row in the {port} section. Count carefully.\n",
    "\"\"\"\n",
    "\n",
    "ports = [\"Mackay\", \"Gladstone\", \"Fisherman Islands\", \"Carrington\", \"Port Kembla\", \"Geelong\", \"Portland\"]\n",
    "\n",
    "print(\"Strategy 2: Extract by port section (Page 1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_port_results = {}\n",
    "for port in ports[:3]:  # Test with first 3 ports\n",
    "    prompt = graincorp_port_prompt.format(port=port)\n",
    "    print(f\"\\nExtracting {port}...\")\n",
    "    result = extract_from_pdf(graincorp_pdf, prompt, page_num=0)\n",
    "    \n",
    "    if isinstance(result, dict):\n",
    "        shipments = result.get(\"shipments\", [])\n",
    "        port_total = result.get(\"port_total\", \"N/A\")\n",
    "        print(f\"  Shipments: {len(shipments)}, Total: {port_total}\")\n",
    "        all_port_results[port] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Row-by-row extraction with explicit counting\n",
    "graincorp_detailed_prompt = \"\"\"\n",
    "This is a VERY DENSE GrainCorp Shipping Stem document.\n",
    "\n",
    "I need you to extract data with EXTREME PRECISION. Follow these rules:\n",
    "\n",
    "1. Count EVERY row in EVERY table section\n",
    "2. Do NOT skip rows even if data is repeated or similar\n",
    "3. Include rows with \"TBA\" vessel names\n",
    "4. Include rows with \"Blank\" or \"(blank)\" values\n",
    "5. Include rows for non-grain items (Cement, Woodchip, BERTH WORKS)\n",
    "\n",
    "Return JSON with this structure:\n",
    "\n",
    "{\n",
    "    \"extraction_stats\": {\n",
    "        \"total_rows_counted\": \"actual count of ALL data rows\",\n",
    "        \"rows_by_month\": {\"Nov 25\": N, \"Dec 25\": N, ...}\n",
    "    },\n",
    "    \"shipments\": [\n",
    "        {\n",
    "            \"row_number\": \"sequential row number for verification\",\n",
    "            \"month\": \"Month\",\n",
    "            \"port\": \"Port\",\n",
    "            \"slot_ref\": \"Slot reference\",\n",
    "            \"exporter\": \"Exporter\",\n",
    "            \"vessel\": \"Vessel name\",\n",
    "            \"commodity\": \"Commodity\",\n",
    "            \"tonnes\": \"Tonnes\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "CRITICAL: The document shows approximately 100+ rows across 3 pages.\n",
    "If you extract fewer than 50 rows from page 1, you are missing data.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Strategy 3: Detailed extraction with row counting (Page 1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "detailed_result = extract_from_pdf(graincorp_pdf, graincorp_detailed_prompt, page_num=0)\n",
    "\n",
    "if isinstance(detailed_result, dict):\n",
    "    stats = detailed_result.get(\"extraction_stats\", {})\n",
    "    shipments = detailed_result.get(\"shipments\", [])\n",
    "    print(f\"Reported row count: {stats.get('total_rows_counted', 'N/A')}\")\n",
    "    print(f\"Actual shipments extracted: {len(shipments)}\")\n",
    "    print(f\"\\nRows by month: {json.dumps(stats.get('rows_by_month', {}), indent=2)}\")\n",
    "else:\n",
    "    print(\"Unexpected format\")\n",
    "    print(str(detailed_result)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ALL pages from GrainCorp with detailed prompt\n",
    "graincorp_bytes = graincorp_pdf.read_bytes()\n",
    "graincorp_pages = file_to_pages(graincorp_pdf.name, graincorp_bytes)\n",
    "\n",
    "print(f\"GrainCorp document has {len(graincorp_pages)} pages\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_graincorp_shipments = []\n",
    "page_stats = []\n",
    "\n",
    "for i in range(len(graincorp_pages)):\n",
    "    print(f\"\\nProcessing page {i+1}/{len(graincorp_pages)}...\")\n",
    "    result = extract_from_pdf(graincorp_pdf, graincorp_detailed_prompt, page_num=i)\n",
    "    \n",
    "    if isinstance(result, dict):\n",
    "        shipments = result.get(\"shipments\", [])\n",
    "        for s in shipments:\n",
    "            s[\"source_page\"] = i + 1\n",
    "            all_graincorp_shipments.append(s)\n",
    "        \n",
    "        stats = result.get(\"extraction_stats\", {})\n",
    "        page_stats.append({\n",
    "            \"page\": i + 1,\n",
    "            \"reported_rows\": stats.get(\"total_rows_counted\"),\n",
    "            \"extracted\": len(shipments)\n",
    "        })\n",
    "        print(f\"  Extracted: {len(shipments)} shipments\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOTAL shipments extracted: {len(all_graincorp_shipments)}\")\n",
    "print(f\"\\nPage statistics:\")\n",
    "for ps in page_stats:\n",
    "    print(f\"  Page {ps['page']}: {ps['extracted']} extracted (reported: {ps['reported_rows']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results Comparison\n",
    "\n",
    "Compare extraction results across document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all extractions\n",
    "print(\"Extraction Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    (\"CBH Shipping Stem\", len(cbh_result.get(\"shipments\", [])) if isinstance(cbh_result, dict) else 0),\n",
    "    (\"Queensland Bulk Terminals\", len(all_qbt_shipments)),\n",
    "    (\"GrainCorp Shipping Stem\", len(all_graincorp_shipments)),\n",
    "]\n",
    "\n",
    "for doc, count in results:\n",
    "    print(f\"{doc}: {count} shipments\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nNOTE: GrainCorp document is very dense.\")\n",
    "print(\"Expected ~100+ shipments across 3 pages.\")\n",
    "print(\"If extraction is significantly lower, consider:\")\n",
    "print(\"  1. Using ColPali for visual similarity search\")\n",
    "print(\"  2. Breaking document into smaller sections\")\n",
    "print(\"  3. Using table-specific extraction tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "For dense documents like GrainCorp:\n",
    "\n",
    "1. **ColPali Indexing**: Index all pages with ColPali embeddings for visual search\n",
    "   ```bash\n",
    "   cocoindex setup\n",
    "   cocoindex update\n",
    "   ```\n",
    "\n",
    "2. **Semantic Search**: Query specific shipments by port, commodity, or vessel\n",
    "   ```python\n",
    "   query_embedding = query_to_colpali_embedding.eval(\"Portland wheat shipments\")\n",
    "   ```\n",
    "\n",
    "3. **Targeted Extraction**: Extract from search results instead of whole document\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [BAML Documentation](https://docs.boundaryml.com)\n",
    "- [CocoIndex Documentation](https://cocoindex.io/docs)\n",
    "- [ColPali Paper](https://arxiv.org/abs/2407.01449)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
